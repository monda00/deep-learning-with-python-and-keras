# 気づき

## ３章：入門ニューラルネットワーク

- 損失関数は最小化する数量
- オプティマイザは更新する方法
- 正しい損失関数を選ぶ
  - 二値分類は交差エントロピー
  - 多クラス分類は多クラス交差エントロピー
  - 回帰問題は平均二乗誤差(mse)
  - 系列学習問題にはCTC
- 損失関数は複数設定できる
- ニューラルネットワークの入力になるようにテンソルに変換
  - リストをパディングして同じ長さに揃える
  - one-hotになるベクトルに変換
- one-hotエンコーディングは、カテゴリ値のデータで広く使われている
- 46種類のクラスを学習するには、16次元の空間では制限がきついかも？？
- categorical_crossentropyは２つの確率分布（ネットワークによって出力されるものと、真の分布）の距離を計測する
- ロジスティック回帰は回帰アルゴリズムではなく、分類アルゴリズム
- 異なる範囲の値をとる特徴量を入力とすると、学習が困難になる
  - 特徴量ごとの正規化が有効（特徴量の平均値を引き、標準偏差で割る）
  - テストデータの正規化は訓練データを使って計算する
- スカラー回帰は連続値を１つ予測する回帰
  - 最後の層を活性化関数を設定しないで、線形の層にする
- k分割交差検証では、k個の検証スコアの平均を求める
- k分割交差検証のKの値は4か5になる
- 各データ点をその手前にあるデータ点の指数移動平均に置き換える
- 回帰での損失関数は平均二乗誤差（MSE）、評価指数は平均絶対誤差（MAE)が使われることが多い
- 訓練データが少ない場合、隠れ層が少ないネットワークが望ましい

## ４章：機械学習の基礎

- データの注意するべき点
  - データの典型性
  - 時間の矢
  - データの冗長性
- ニューラルネットワークのデータ前処理
  - ベクトル化
  - 正規化
    - 小さな値をとる
    - 種類が同じ
    - 平均が０、標準偏差が１
  - 欠損値の処理
  - 特徴抽出
    - 特徴量をより単純な方法で表現することで問題を容易にする
- 層の数や各層のサイズをわりだす公式はない
  - 比較的少ない数の層とパラメータから始めて、検証データセットでの損失値に関して収穫逓減がみられるまで、層のサイズを大きくするか、新しい層を追加する
- ネットワークのキャパシティ（学習可能なパラメータ）が大きいほど訓練データは素早くモデル化されるが、その分、過学習に陥り易くなる
- L2正則化では追加するコストは重み係数の値の二乗に比例する
  - ニューラルネットワークでは荷重減衰とも呼ばれる
- ドロップアウト率は通常は0.2から0.5に設定される
- まずは層を追加、層を大きく、エポック数を増やすなどをして、モデルを過学習させることで、必要なモデルの大きさを突き止める
  - 過学習したモデルからチューニングをして理想的なモデルに近づける

## ５章：コンピュータビジョンのためのディープラーニング

- CNNの入力テンソルは(image_height, image_width, image_channels)
- ３次元の出力を１次元に平坦化する
- RGB画像の場合、カラーチャネルは３つなので、深さ軸は３になる
- 畳み込み層の３つ目の軸はフィルタ
- 特徴マップは、畳み込み演算により入力特徴マップからパッチを抽出し、それら全てのパッチに同じ変換を適用したもの
  - フィルタによる応答マップの集まり
- 畳み込み層は、入力から抽出されたパッチのサイズと出力特徴マップの深さをパラメータにもつ
- マッチサイズは通常、3×3または5×5
- 特徴マップのダウンサンプリングでは、最大値プーリングが使用される
  - 最終的な特徴マップが非常に大きくなってしまう
- 小さなデータセットにディープラーニングを適用するために基本的な手法
  - 学習済みのモデルによる特徴抽出
  - 学習済みのモデルのファインチューニング
- 大規模なデータセットで訓練したモデルを少し変更するだけで、異なる問題に再利用できる
- 大きな画像を扱うときは、ネットワークのキャパシティを増強し、Flatten層に到達した時の特徴マップが大きくなりすぎないようにする
- 深さが増え、サイズが減るのCNNでよくみるパターン
- generatorを受け取るfit_generatorがある
- kerasでデータ拡張するときは、ImageDataGeneratorクラスをインスタンス化する
- 学習曲線と検証曲線が追従している＝過学習に陥っていない
- データ拡張し、データ数を増やすことで過学習を抑えることができる
- 学習データが小さいときは、学習済みネットワークを利用する
- ディープラーニングの学習済みモデルには、様々な画像を識別することが可能
- 学習済みネットワークを使用する方法は２つある
  - 特徴抽出
    - １つ前のネットワークが学習した表現に基づいて、新しいサンプルから興味深い特徴量を抽出する
  - ファインチューニング
    - 特徴抽出に使用される凍結された畳み込みベースの出力側の層をいくつか解凍し、モデルの新しく追加された部分と解凍した層の両方で訓練を行う
- CNNの特徴抽出では、畳み込みベース（プーリング層と畳み込み層）を再利用する
  - 全結合分類器の再利用は避ける
  - 畳み込み層の特徴マップは画像に対する一般概念
- モデルの最初の方にある層は、高い局所的な特徴マップ（エッジ、色など）
- 最後の方の層は、より抽象的な概念（猫の耳など）
- 特徴抽出では２つの方法がある
  - データ拡張を行わない高速な方法
    - 新しいデータセットで畳み込みベースを実行し、その出力を次のモデルの入力として使用
  - データ拡張を行う方法
    - 低速でコストがかかる
    - 層を追加しモデルを拡張する
    - 畳み込みベースの凍結をする（重みを更新しないように）
- ファイルチューニングの手順
  1. 訓練済みのベースネットワークの最後にカスタムネットワークを追加
  2. ベースネットワークを凍結する
  3. 追加した部分の訓練を行う
  4. ベースネットワークの一部の層を解凍する
  5. 解凍した層と追加した部分の訓練を同時に行う
- ファインチューニングでは畳み込みベース全体に行う訳では無い
  - 出力側の層は具体的な特徴量をエンコードしているため、新しい問題では解凍する
- ノイズだらけの曲線は、各データ点をその手前にあるデータ点の指数移動平均に置き換える
- CNNの学習は可視化しやすい
  - CNNの中間出力の可視化
  - CNNのフィルタの可視化
  - 画像におけるクラス活性化のヒートマップの可視化
- 中間層の活性化を出力するモデルを作成する
- フィルタが空であることは、そのフィルタにエンコードされているパターンが入力画像から検出されていないことを意味する
- フィルタの可視化
  - フィルタが応答することになっている視覚パターンを表示する
    - 空の入力画像から始めて、CNNの入力画像の値に勾配降下法を適用し、特定のフィルタの応答を最大する
  - 入力画像からクラス活性化のヒートマップを生成する（CAMと呼ばれる）
    - 画像のどの部分がCNNの最終的な分類の決め手になったかを理解するのに役立つ

## ６章：テキストとシーケンスのためのディープラーニング

- テキスト、時系列データ、シーケンスデータ全般を処理するディープラーニングは２つ
  - RNN
  - １次元のCNN
- テキストに分割に使用できる単位（単語、文字、Nグラム）をトークンという
- 主なトークン化の手法は、one-hotエンコーディングとトークン埋め込み
  - one-hotは次元の高い二値の疎ベクトル
  - 単語埋め込みは低次元の浮動小数点の密ベクトルであり、データから学習される
    - メインのタスク（分類など）と同時に単語埋め込みを学習する
    - 別の機械学習タスクを使って計算された単語埋め込みをモデルに読み込む
- 単語のインデックス→埋め込み層→対応する単語ベクトル
- KerasのEmbedding層を使って、タスクに特化したトークン埋め込みを学習する
- フィードフォワードネットワークとは、１つのベクトルを一度に処理するもの
- RNNは内部ループをもつNN
- KerasのsimpleRNNの入力の形状は(batch_size, timesteps, input_features)
- simpleRNNは２種類のモードで実行できる
  - 各時間刻みの出力が順番に含まれた完全なシーケンスを返す
  - 各入力シーケンスの最後の出力だけを返す
- 複数のリカレント層を積み重ねると表現力が高くなることがある
  - 完全な出力シーケンスを取得するために中間の層の出力を全て取得(return_sequences=True)する必要がある
- LSTMセルは過去の情報を後から再注入できるようにすることで勾配消失問題に対処する
  - LSTMのアーキテクチャを全て理解することより、何ができるかを理解することが大事
- Pythonジェネレータを使うことで、元のデータを使ってその場でサンプルを生成できる
  - 各サンプルを明示的に持っているのは無駄になる
- RNNなどの計算不可の高いモデルを調べる際には、単純で計算不可の少ないモデルを試す
- GRU層はLSTM層より実行コストがかからないが、表現力が劣ることがある
- RNN層の前にドロップアウト層を入れると学習の妨げになる
  - 全ての時間刻みで同じドロップアウトマスクを適用する
  - dropoutパラメータはその層の入力ユニットのドロップアウト率を指定
  - recurrent_dropoutパラメータはリカレントユニットのドロップアウト率を指定
- 過学習に陥らず、性能にボトルネックがあるときは、過学習に陥るまではキャパシティを増やす
  - 層を増やしても過学習が酷くないときは、まだ増やせる、もしくはキャパシティ以外の方法を考える
- 双方向RNNは万能
  - RNNは順序に依存するため、双方向RNNを利用することで順序により見落とされるパータンを捕捉できる
- 何をすべきかの理論は存在しないため、実装して試す
- CNNはコンピュータビジョンの問題だけでなく、シーケンス処理のも適用できる
  - RNNのライバルになるうる
  - 通常はRNNに比べて計算コストがかからない
- １次元CNNでは入力パッチを個別に処理するため、時間刻みの順序に敏感ではない
- CNNの軽快さとRNNの順序への敏感さを組み合わせる手法の１つは、１次元CNNをRNNの前処理ステップとして使用すること
  - 役にたつ場面は、数千もの時間刻みからなるシーケンスなど、RNNで処理するのが現実的ではないほど長いシーケンスを扱っている場合

## ７章：高度なディープラーニングのベストプラクティス

- Sequentialモデルでは１つの入力、１つの出力のモデルしか表現できない
- マルチモーダル入力（テキスト、メタデータ、画像などを１つの入力とする）なものもある
  - 別々に学習して合わせる方法もあるが、同時に学習する方がいい
- Functional APIでは、多入力モデル、多出力モデル、グラフ形式のモデルを表現できる
- 多入力モデルは複数の入力をマージする
- 多出力モデルはそれぞれの出力に損失関数を設定する
- 有向非循環グラフを実装できる
  - Inceptionモジュール
    - CNN用のよく知られているネットワークアーキテクチャ
  - 残差接続
    - グラフ形式のネットワークコンポーネント

# 疑問

- 層の数、ユニットの数、損失関数、活性化関数は全ての組み合わせを調べないとどれがいいのかわからない？
  - 小さい値から地道に調べていく
- 分類でのk-foldはどうすれば？
- batchサイズはどう決める？
- L1正則化とL2正則化の使い分けは？
- どこにDropout層を入れればいいのか
- 出力特徴マップの深さはどう決める？
- CNNでConv2DとMaxPooling2Dで一組なのか？５章では、Conv2DとMaxPooling2Dで終わるパターンそれぞれある
- 可視化における中間層の出力とフィルタの違い
- フィルタの可視化の原理（視覚パターンとヒートマップ）
- 画像以外に対するCNNは？
- １次元CNNと２次元CNNの違いのウィンドウの余裕とは？
- CNN+RNNのモデルは長いシーケンスを扱うが、長いシーケンスをそのまま入力とするのか？
- 内部ネットワークトポロジの活用事例
- 同じ層、モデルのインスタンスを複数回呼び出すメリット

