# 気づき

## ３章：入門ニューラルネットワーク

- 損失関数は最小化する数量
- オプティマイザは更新する方法
- 正しい損失関数を選ぶ
  - 二値分類は交差エントロピー
  - 多クラス分類は多クラス交差エントロピー
  - 回帰問題は平均二乗誤差(mse)
  - 系列学習問題にはCTC
- 損失関数は複数設定できる
- ニューラルネットワークの入力になるようにテンソルに変換
  - リストをパディングして同じ長さに揃える
  - one-hotになるベクトルに変換
- one-hotエンコーディングは、カテゴリ値のデータで広く使われている
- 46種類のクラスを学習するには、16次元の空間では制限がきついかも？？
- categorical_crossentropyは２つの確率分布（ネットワークによって出力されるものと、真の分布）の距離を計測する
- ロジスティック回帰は回帰アルゴリズムではなく、分類アルゴリズム
- 異なる範囲の値をとる特徴量を入力とすると、学習が困難になる
  - 特徴量ごとの正規化が有効（特徴量の平均値を引き、標準偏差で割る）
  - テストデータの正規化は訓練データを使って計算する
- スカラー回帰は連続値を１つ予測する回帰
  - 最後の層を活性化関数を設定しないで、線形の層にする
- k分割交差検証では、k個の検証スコアの平均を求める
- k分割交差検証のKの値は4か5になる
- 各データ点をその手前にあるデータ点の指数移動平均に置き換える
- 回帰での損失関数は平均二乗誤差（MSE）、評価指数は平均絶対誤差（MAE)が使われることが多い
- 訓練データが少ない場合、隠れ層が少ないネットワークが望ましい

## ４章：機械学習の基礎

- データの注意するべき点
  - データの典型性
  - 時間の矢
  - データの冗長性
- ニューラルネットワークのデータ前処理
  - ベクトル化
  - 正規化
    - 小さな値をとる
    - 種類が同じ
    - 平均が０、標準偏差が１
  - 欠損値の処理
  - 特徴抽出
    - 特徴量をより単純な方法で表現することで問題を容易にする
- 層の数や各層のサイズをわりだす公式はない
  - 比較的少ない数の層とパラメータから始めて、検証データセットでの損失値に関して収穫逓減がみられるまで、層のサイズを大きくするか、新しい層を追加する
- ネットワークのキャパシティ（学習可能なパラメータ）が大きいほど訓練データは素早くモデル化されるが、その分、過学習に陥り易くなる

# 疑問

- 層の数、ユニットの数、損失関数、活性化関数は全ての組み合わせを調べないとどれがいいのかわからない？
  - 小さい値から地道に調べていく
- 分類でのk-foldはどうすれば？
- batchサイズはどう決める？

