# 気づき

## ３章：入門ニューラルネットワーク

- 損失関数は最小化する数量
- オプティマイザは更新する方法
- 正しい損失関数を選ぶ
  - 二値分類は交差エントロピー
  - 多クラス分類は多クラス交差エントロピー
  - 回帰問題は平均二乗誤差(mse)
  - 系列学習問題にはCTC
- 損失関数は複数設定できる
- ニューラルネットワークの入力になるようにテンソルに変換
  - リストをパディングして同じ長さに揃える
  - one-hotになるベクトルに変換
- one-hotエンコーディングは、カテゴリ値のデータで広く使われている
- 46種類のクラスを学習するには、16次元の空間では制限がきついかも？？
- categorical_crossentropyは２つの確率分布（ネットワークによって出力されるものと、真の分布）の距離を計測する
- ロジスティック回帰は回帰アルゴリズムではなく、分類アルゴリズム
- 異なる範囲の値をとる特徴量を入力とすると、学習が困難になる
  - 特徴量ごとの正規化が有効（特徴量の平均値を引き、標準偏差で割る）
  - テストデータの正規化は訓練データを使って計算する
- スカラー回帰は連続値を１つ予測する回帰
  - 最後の層を活性化関数を設定しないで、線形の層にする
- k分割交差検証では、k個の検証スコアの平均を求める
- k分割交差検証のKの値は4か5になる
- 各データ点をその手前にあるデータ点の指数移動平均に置き換える
- 回帰での損失関数は平均二乗誤差（MSE）、評価指数は平均絶対誤差（MAE)が使われることが多い
- 訓練データが少ない場合、隠れ層が少ないネットワークが望ましい

## ４章：機械学習の基礎

- データの注意するべき点
  - データの典型性
  - 時間の矢
  - データの冗長性
- ニューラルネットワークのデータ前処理
  - ベクトル化
  - 正規化
    - 小さな値をとる
    - 種類が同じ
    - 平均が０、標準偏差が１
  - 欠損値の処理
  - 特徴抽出
    - 特徴量をより単純な方法で表現することで問題を容易にする
- 層の数や各層のサイズをわりだす公式はない
  - 比較的少ない数の層とパラメータから始めて、検証データセットでの損失値に関して収穫逓減がみられるまで、層のサイズを大きくするか、新しい層を追加する
- ネットワークのキャパシティ（学習可能なパラメータ）が大きいほど訓練データは素早くモデル化されるが、その分、過学習に陥り易くなる
- L2正則化では追加するコストは重み係数の値の二乗に比例する
  - ニューラルネットワークでは荷重減衰とも呼ばれる
- ドロップアウト率は通常は0.2から0.5に設定される
- まずは層を追加、層を大きく、エポック数を増やすなどをして、モデルを過学習させることで、必要なモデルの大きさを突き止める
  - 過学習したモデルからチューニングをして理想的なモデルに近づける

## ５章：コンピュータビジョンのためのディープラーニング

- CNNの入力テンソルは(image_height, image_width, image_channels)
- ３次元の出力を１次元に平坦化する
- RGB画像の場合、カラーチャネルは３つなので、深さ軸は３になる
- 畳み込み層の３つ目の軸はフィルタ
- 特徴マップは、畳み込み演算により入力特徴マップからパッチを抽出し、それら全てのパッチに同じ変換を適用したもの
  - フィルタによる応答マップの集まり
- 畳み込み層は、入力から抽出されたパッチのサイズと出力特徴マップの深さをパラメータにもつ
- マッチサイズは通常、3×3または5×5
- 特徴マップのダウンサンプリングでは、最大値プーリングが使用される
  - 最終的な特徴マップが非常に大きくなってしまう

# 疑問

- 層の数、ユニットの数、損失関数、活性化関数は全ての組み合わせを調べないとどれがいいのかわからない？
  - 小さい値から地道に調べていく
- 分類でのk-foldはどうすれば？
- batchサイズはどう決める？
- L1正則化とL2正則化の使い分けは？
- どこにDropout層を入れればいいのか
- 出力特徴マップの深さはどう決める？

