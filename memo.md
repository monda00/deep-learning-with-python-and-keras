# 気づき

## ３章：入門ニューラルネットワーク

- 損失関数は最小化する数量
- オプティマイザは更新する方法
- 正しい損失関数を選ぶ
  - 二値分類は交差エントロピー
  - 多クラス分類は多クラス交差エントロピー
  - 回帰問題は平均二乗誤差(mse)
  - 系列学習問題にはCTC
- 損失関数は複数設定できる
- ニューラルネットワークの入力になるようにテンソルに変換
  - リストをパディングして同じ長さに揃える
  - one-hotになるベクトルに変換
- one-hotエンコーディングは、カテゴリ値のデータで広く使われている
- 46種類のクラスを学習するには、16次元の空間では制限がきついかも？？
- categorical_crossentropyは２つの確率分布（ネットワークによって出力されるものと、真の分布）の距離を計測する
- ロジスティック回帰は回帰アルゴリズムではなく、分類アルゴリズム
- 異なる範囲の値をとる特徴量を入力とすると、学習が困難になる
  - 特徴量ごとの正規化が有効（特徴量の平均値を引き、標準偏差で割る）
  - テストデータの正規化は訓練データを使って計算する
- スカラー回帰は連続値を１つ予測する回帰
  - 最後の層を活性化関数を設定しないで、線形の層にする
- k分割交差検証では、k個の検証スコアの平均を求める
- k分割交差検証のKの値は4か5になる
- 各データ点をその手前にあるデータ点の指数移動平均に置き換える
- 回帰での損失関数は平均二乗誤差（MSE）、評価指数は平均絶対誤差（MAE)が使われることが多い
- 訓練データが少ない場合、隠れ層が少ないネットワークが望ましい

## ４章：機械学習の基礎

- データの注意するべき点
  - データの典型性
  - 時間の矢
  - データの冗長性
- ニューラルネットワークのデータ前処理
  - ベクトル化
  - 正規化
    - 小さな値をとる
    - 種類が同じ
    - 平均が０、標準偏差が１
  - 欠損値の処理
  - 特徴抽出
    - 特徴量をより単純な方法で表現することで問題を容易にする
- 層の数や各層のサイズをわりだす公式はない
  - 比較的少ない数の層とパラメータから始めて、検証データセットでの損失値に関して収穫逓減がみられるまで、層のサイズを大きくするか、新しい層を追加する
- ネットワークのキャパシティ（学習可能なパラメータ）が大きいほど訓練データは素早くモデル化されるが、その分、過学習に陥り易くなる
- L2正則化では追加するコストは重み係数の値の二乗に比例する
  - ニューラルネットワークでは荷重減衰とも呼ばれる
- ドロップアウト率は通常は0.2から0.5に設定される
- まずは層を追加、層を大きく、エポック数を増やすなどをして、モデルを過学習させることで、必要なモデルの大きさを突き止める
  - 過学習したモデルからチューニングをして理想的なモデルに近づける

## ５章：コンピュータビジョンのためのディープラーニング

- CNNの入力テンソルは(image_height, image_width, image_channels)
- ３次元の出力を１次元に平坦化する
- RGB画像の場合、カラーチャネルは３つなので、深さ軸は３になる
- 畳み込み層の３つ目の軸はフィルタ
- 特徴マップは、畳み込み演算により入力特徴マップからパッチを抽出し、それら全てのパッチに同じ変換を適用したもの
  - フィルタによる応答マップの集まり
- 畳み込み層は、入力から抽出されたパッチのサイズと出力特徴マップの深さをパラメータにもつ
- マッチサイズは通常、3×3または5×5
- 特徴マップのダウンサンプリングでは、最大値プーリングが使用される
  - 最終的な特徴マップが非常に大きくなってしまう
- 小さなデータセットにディープラーニングを適用するために基本的な手法
  - 学習済みのモデルによる特徴抽出
  - 学習済みのモデルのファインチューニング
- 大規模なデータセットで訓練したモデルを少し変更するだけで、異なる問題に再利用できる
- 大きな画像を扱うときは、ネットワークのキャパシティを増強し、Flatten層に到達した時の特徴マップが大きくなりすぎないようにする
- 深さが増え、サイズが減るのCNNでよくみるパターン
- generatorを受け取るfit_generatorがある
- kerasでデータ拡張するときは、ImageDataGeneratorクラスをインスタンス化する
- 学習曲線と検証曲線が追従している＝過学習に陥っていない
- データ拡張し、データ数を増やすことで過学習を抑えることができる
- 学習データが小さいときは、学習済みネットワークを利用する
- ディープラーニングの学習済みモデルには、様々な画像を識別することが可能
- 学習済みネットワークを使用する方法は２つある
  - 特徴抽出
    - １つ前のネットワークが学習した表現に基づいて、新しいサンプルから興味深い特徴量を抽出する
  - ファインチューニング
    - 特徴抽出に使用される凍結された畳み込みベースの出力側の層をいくつか解凍し、モデルの新しく追加された部分と解凍した層の両方で訓練を行う
- CNNの特徴抽出では、畳み込みベース（プーリング層と畳み込み層）を再利用する
  - 全結合分類器の再利用は避ける
  - 畳み込み層の特徴マップは画像に対する一般概念
- モデルの最初の方にある層は、高い局所的な特徴マップ（エッジ、色など）
- 最後の方の層は、より抽象的な概念（猫の耳など）
- 特徴抽出では２つの方法がある
  - データ拡張を行わない高速な方法
    - 新しいデータセットで畳み込みベースを実行し、その出力を次のモデルの入力として使用
  - データ拡張を行う方法
    - 低速でコストがかかる
    - 層を追加しモデルを拡張する
    - 畳み込みベースの凍結をする（重みを更新しないように）
- ファイルチューニングの手順
  1. 訓練済みのベースネットワークの最後にカスタムネットワークを追加
  2. ベースネットワークを凍結する
  3. 追加した部分の訓練を行う
  4. ベースネットワークの一部の層を解凍する
  5. 解凍した層と追加した部分の訓練を同時に行う
- ファインチューニングでは畳み込みベース全体に行う訳では無い
  - 出力側の層は具体的な特徴量をエンコードしているため、新しい問題では解凍する
- ノイズだらけの曲線は、各データ点をその手前にあるデータ点の指数移動平均に置き換える
- CNNの学習は可視化しやすい
  - CNNの中間出力の可視化
  - CNNのフィルタの可視化
  - 画像におけるクラス活性化のヒートマップの可視化
- 中間層の活性化を出力するモデルを作成する
- フィルタが空であることは、そのフィルタにエンコードされているパターンが入力画像から検出されていないことを意味する
- フィルタの可視化
  - フィルタが応答することになっている視覚パターンを表示する
    - 空の入力画像から始めて、CNNの入力画像の値に勾配降下法を適用し、特定のフィルタの応答を最大する
  - 入力画像からクラス活性化のヒートマップを生成する（CAMと呼ばれる）
    - 画像のどの部分がCNNの最終的な分類の決め手になったかを理解するのに役立つ

## ６章：テキストとシーケンスのためのディープラーニング

- テキスト、時系列データ、シーケンスデータ全般を処理するディープラーニングは２つ
  - RNN
  - １次元のCNN
- テキストに分割に使用できる単位（単語、文字、Nグラム）をトークンという
- 主なトークン化の手法は、one-hotエンコーディングとトークン埋め込み
  - one-hotは次元の高い二値の疎ベクトル
  - 単語埋め込みは低次元の浮動小数点の密ベクトルであり、データから学習される
    - メインのタスク（分類など）と同時に単語埋め込みを学習する
    - 別の機械学習タスクを使って計算された単語埋め込みをモデルに読み込む
- 単語のインデックス→埋め込み層→対応する単語ベクトル
- KerasのEmbedding層を使って、タスクに特化したトークン埋め込みを学習する
- フィードフォワードネットワークとは、１つのベクトルを一度に処理するもの
- RNNは内部ループをもつNN
- KerasのsimpleRNNの入力の形状は(batch_size, timesteps, input_features)
- simpleRNNは２種類のモードで実行できる
  - 各時間刻みの出力が順番に含まれた完全なシーケンスを返す
  - 各入力シーケンスの最後の出力だけを返す
- 複数のリカレント層を積み重ねると表現力が高くなることがある
  - 完全な出力シーケンスを取得するために中間の層の出力を全て取得(return_sequences=True)する必要がある
- LSTMセルは過去の情報を後から再注入できるようにすることで勾配消失問題に対処する
  - LSTMのアーキテクチャを全て理解することより、何ができるかを理解することが大事
- Pythonジェネレータを使うことで、元のデータを使ってその場でサンプルを生成できる
  - 各サンプルを明示的に持っているのは無駄になる
- RNNなどの計算不可の高いモデルを調べる際には、単純で計算不可の少ないモデルを試す
- GRU層はLSTM層より実行コストがかからないが、表現力が劣ることがある
- RNN層の前にドロップアウト層を入れると学習の妨げになる
  - 全ての時間刻みで同じドロップアウトマスクを適用する
  - dropoutパラメータはその層の入力ユニットのドロップアウト率を指定
  - recurrent_dropoutパラメータはリカレントユニットのドロップアウト率を指定
- 過学習に陥らず、性能にボトルネックがあるときは、過学習に陥るまではキャパシティを増やす
  - 層を増やしても過学習が酷くないときは、まだ増やせる、もしくはキャパシティ以外の方法を考える
- 双方向RNNは万能
  - RNNは順序に依存するため、双方向RNNを利用することで順序により見落とされるパータンを捕捉できる
- 何をすべきかの理論は存在しないため、実装して試す
- CNNはコンピュータビジョンの問題だけでなく、シーケンス処理のも適用できる
  - ２次元CNNは２次元空間での資格パターンの処理に適している
  - １次元CNNは時間的なパターンの処理に適している
  - RNNのライバルになるうる
  - 通常はRNNに比べて計算コストがかからない
- １次元CNNでは入力パッチを個別に処理するため、時間刻みの順序に敏感ではない
- CNNの軽快さとRNNの順序への敏感さを組み合わせる手法の１つは、１次元CNNをRNNの前処理ステップとして使用すること
  - 役にたつ場面は、数千もの時間刻みからなるシーケンスなど、RNNで処理するのが現実的ではないほど長いシーケンスを扱っている場合

## ７章：高度なディープラーニングのベストプラクティス

- Sequentialモデルでは１つの入力、１つの出力のモデルしか表現できない
- マルチモーダル入力（テキスト、メタデータ、画像などを１つの入力とする）なものもある
  - 別々に学習して合わせる方法もあるが、同時に学習する方がいい
- Functional APIでは、多入力モデル、多出力モデル、グラフ形式のモデルを表現できる
- 多入力モデルは複数の入力をマージする
- 多出力モデルはそれぞれの出力に損失関数を設定する
- 有向非循環グラフを実装できる
  - ニューラルネットワークに共通する構成要素の一部は、グラフとしてKerasで実装されている
  - Inceptionモジュール
    - CNN用のよく知られているネットワークアーキテクチャ
  - 残差接続
    - グラフ形式のネットワークコンポーネント
- callbackを使うことで、検証データでの損失関数の改善が認められなくなった時点で学習を止められる
- callbackの使用例
  - モデルのチェックポイント化
  - 訓練の中止
  - 特定パラメータの動的な調整
  - 訓練と検証の指標を記録
- TensorBoardは、訓練中にモデルの内部で起きていることを全て監視できる
  - 訓練中に指標を視覚的に監視
  - モデルのアーキテクチャの可視化
  - 活性化と勾配のヒストグラムの可視化
  - 埋め込みを３次元で調査
- 高度なアーキテクチャパターン
  - バッチ正規化
    - 層の一種（BatchNormalization）
    - バッチごとのデータの平均と分散の指数移動平均を内部で維持する
    - 勾配の伝播を手助けすることで、ネットワークを深くすることが可能に
    - バッチ再正規化、自己正規化ニューラルネットワークなども提唱されている（有益性はまだ広く再現されていない）
  - dw畳み込み
    - 入力の各チャネルで空間畳み込み演算を別々に実行し、出力チャネルを連結
    - Conv2Dに置き換わる（モデルが軽量になり、高速になる）
    - 入力の空間的な位置同士は高い相関関係にあるが、異なるチャネル同士はほぼ独立している場合に有効
- 入力するデータが正規化されていても、出力されるデータが正規化されているとは限らない
- ハイパーパラメータのチューニングに正式なルールはない（エンジニアや研究者は直感を養っている）
- ハイパーパラメータを最適化するプロセス
  1. ハイパーパラメータの集まりを（自動的に）選択する
  2. 対応するモデルを構築する
  3. モデルに訓練データを学習させ、検証データで最終的な性能を測定する
  4. 次に試すハイパーパラメータの集まりを（自動的に）選択する
  5. 手順2~3を繰り返す
  6. テストデータでモデルの性能を測定する
  - 次のハイパーパラメータを選択するアルゴリズムは、ベイズ最適化、遺伝的アルゴリズム、ランダムサーチなどがある
  - 一般に効果的だと言われているのはランダムサーチ
  - 著者のオススメはHyperoptというライブラリ（Keras用にHyperasというライブラリもある）
- アンサンブルでは、別々に訓練されたモデルがそれぞれに良いモデルであると考えられることが前提
- 分類器のアンサンブルを作成するいい方法は、荷重平均を求める
- アンサンブルを成功させる鍵は多様性
  - 出来るだけ良いモデル、かつ、出来るだけ異なるモデル

## ８章：ジェネレーティブディープラーニング

- RNNを使ってシーケンスの生成が可能
  - 同じ手法であらゆる種類のシーケンスを生成可能
- テキスト生成のサンプリング戦略は重要
  - 貪欲的サンプリングだと興味深いものは生成されにくい
  - 確率的サンプリングがいい
- テキスト生成の流れ
  1. それまでに生成されたテキストを元に、モデルから次の文字の確率分布を抽出
  2. この確率分布を特定の温度で再荷重
  3. 再荷重された確率分布に従って、次の文字をサンプリング
  4. それまでに生成されたテキストの最後に新しい文字を追加
- ソフトマックスの温度を制御することで、サンプリングに使用される確率分布のエントロピーを特徴づけることが可能
  - 次の文字の選択をどれくらい意外なものにするか制御できる（高いほど意外なものになる）
- DeepDreamはCNNによって学習された表現に基づく画像加工手法
  - CNNのフィルタ可視化手法とほぼ同じ
  - 層全体の活性化を最大化する
  - 最大化の対象は損失値
- スタイル（画風など）とコンテンツ（画像の内容）が数学的に定義できるとスタイル変換ができる
  - A Neural Algorithm of Artistic Styleの論文での内容
  - コンテンツは出力側の層（画像のコンテンツをより大域的に補足しているはず）を活用
  - スタイルは様々な層の活性化に含まれる相関関係を利用する
  - 信号処理に近いもので、魔法のような効果を期待するものではない
- 潜在空間からの点のサンプリングとデコーディングにより、新たな画像を生成できる
  - VAEは、構造化された連続的な潜在空間が得られる
    - 顔を入れ替える、表情を変える、あらゆる種類の画像編集を潜在空間で行うのに適している
  - GANは、リアルなシングルフレーム画像の生成が可能
    - 連続をもつ潜在空間に結びつかないことがある
- GANは生成者ネットワークと判別者ネットワークが相手を打ち負かすように訓練する
  - GANの実装は難しい
  - いくつかの実装上のヒントがある
  - 損失関数が動的になるため訓練が難しい

## ９章：まとめ

- ディープラーニングは今の技術でも十分産業に革命をもたらすことが可能
- ディープラーニングが流行った要因
  - アルゴリズムの漸進的なイノベーション
  - 知覚データの大量供給
  - ハードウェアの進化
  - ソフトウェアの充実
- 検証データでの性能を元に少しずつハイパーパラメータの改善を行う
  - ハイパーパラメータのチューニングが検証データの過学習に注意する
- ディープラーニングの限界を超えたもの
  - 科学的手法のプログラミングや応用といった論理的思考
  - 長期的な計画
  - アルゴリズム的なデータの操作を必要とする問題
  - ソートアルゴリズムの学習ですら途方もなく難しい
- ディープラーニングのモデルは入力を（人間の感覚でいう意味の）理解していない
  - 訓練データの入力値を目的値に１ずつマッピングしているのすぎない
- 人間は既知の概念を組み合わせることで、経験したことのないものを表現できる
- 著者が考えるディープラーニングの中期的な未来
  - プログラムとしてのモデル
  - 新しい学習形態
    - 微分による訓練以外の手法
  - エンジニアの関与をそれほど必要としないモデル
    - ハイパーパラメータのチューニングを自動化
      - 学習をしながらのチューニングも可能になるかも
    - データの前処理は自動化は難しい
  - 学習済みの特徴量やアーキテクチャのより積極的な再利用
    - 学習済みの特徴量、モデルのアーキテクチャ、訓練手続きなどの利用も
    - ライブラリのようになるのかも
- arXivで論文調査
  - 論文は国際会議での査読を待たずにアップロードされている
  - 査読されていないため、重要な論文を探すのは困難

# 疑問

- 層の数、ユニットの数、損失関数、活性化関数は全ての組み合わせを調べないとどれがいいのかわからない？
  - 小さい値から地道に調べていく
- 分類でのk-foldはどうすれば？
  - 回帰と同じ
- batchサイズはどう決める？
  - 大きくすると学習が早くなる
  - 小さくすると１つのデータに対する反応が大きくなる
  - 他のパラメータの変更のときに学習を高速化させるために大きくする
- L1正則化とL2正則化の使い分けは？
  - L1正則化は不要なパラメータを削る
  - L2正則化は過学習を抑える
- どこにDropout層を入れればいいのか
  - 入力に0.2、隠れ層に0.5がベストプラクティスとされている
- 出力特徴マップの深さはどう決める？
  - 深さは特徴マップの種類
- CNNでConv2DとMaxPooling2Dで一組なのか？５章では、Conv2DとMaxPooling2Dで終わるパターンそれぞれある
  - 変化やノイズに対して普遍性を保つためにプーリングしている
  - 計算量を減らせる
  - やったほうがいいが、やらなくても？
- 可視化における中間層の出力とフィルタの違い
  - 中間層の出力は入力画像が層を通過するごとにどうなっていくかを可視化
  - フィルタは画像の何に反応しているかを可視化
- 画像以外に対するCNNは？
  - 時系列データ
  - 自然言語処理
  - ゲーム（AlphaGo)
- CNN+RNNのモデルは長いシーケンスを扱うが、長いシーケンスをそのまま入力とするのか？
  - RNNで処理できないような長いシーケンスをCNNを使って畳み込みしている
  - そのまま入力
- 同じ層、モデルのインスタンスを複数回呼び出すメリット
  - 入力としてディアルカメラを使用するビジョンモデル
  - ２つのカメラからの入力を共有した重みを利用
- 画像の潜在空間とは
  - 画像を生成する空間

